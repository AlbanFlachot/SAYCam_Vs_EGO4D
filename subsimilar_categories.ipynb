{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import importlib\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from matplotlib import pyplot as plt\n",
    "from os.path import join\n",
    "import os\n",
    "import seaborn as sns\n",
    "from torchvision.ops.misc import interpolate\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "#### Custum libraries\n",
    "import lib.algos_maxRSA as max_rsa\n",
    "import lib.utils_RSA as rsa\n",
    "import lib.utils_CKA as cka\n",
    "from lib.algos import *\n",
    "\n",
    "\n",
    "importlib.reload(rsa)\n",
    "importlib.reload(cka)\n",
    "importlib.reload(max_rsa)"
   ],
   "id": "93e9f31bd96905b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dataset = 'ecoLennyTest'\n",
    "models  = ['ego', 'saycam', 'imagenet', 'supervised', 'resnet']\n",
    "#models  = ['ego', 'saycam']\n",
    "path2activations = f'/home/alban/Documents/activations_datadriven/%s_{dataset}/'\n",
    "\n",
    "imagelists = {}\n",
    "activations = {}\n",
    "for model in models:\n",
    "    with open(join(path2activations%model, 'imagepaths.txt'), 'r') as f:\n",
    "        imagelists[model] = [line.strip() for line in f.readlines()]\n",
    "    activations[model] = np.load(join(path2activations % model, 'cls_tokens.npy'))\n",
    "\n",
    "imagelist = imagelists[model]\n",
    "activations[model].shape"
   ],
   "id": "dda873529bcfd1a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#### Normalize vectors\n",
    "for model in models:\n",
    "    norms = np.linalg.norm(activations[model], axis=1, keepdims=True)\n",
    "    activations[model] = activations[model]/norms # normalization"
   ],
   "id": "98c15462363182e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "### check if images were shown in the same order\n",
    "assert imagelists[models[0]] == imagelists[models[1]]\n",
    "imagelist = imagelists[models[0]] # since they are the same, only consider one list\n",
    "\n",
    "#### check if each category has the same number of images and list all categories in listcats\n",
    "count = 0\n",
    "cat = ''\n",
    "listcat = list()\n",
    "for i, imgp in enumerate(imagelist):\n",
    "    current_cat = imgp.split('/')[-2]\n",
    "    if i == 0:\n",
    "        cat = current_cat\n",
    "        listcat.append(current_cat)\n",
    "    if cat != current_cat:\n",
    "        cat = current_cat\n",
    "        listcat.append(current_cat)\n",
    "        count = 1\n",
    "    else:\n",
    "        count += 1\n",
    "\n",
    "nb_per_cat = count # in val, 50 images per cate\n",
    "\n",
    "nb_per_cat"
   ],
   "id": "204242663570ea70",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "### reshape activations according to include categories\n",
    "cat_activations = activations.copy()\n",
    "\n",
    "for model in models:\n",
    "    shape = activations[model].shape\n",
    "    cat_activations[model] = activations[model].reshape(-1, nb_per_cat, shape[-1])"
   ],
   "id": "991ad3c111deb76",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "compactness, compact_categories = max_rsa.compute_compactness(cat_activations, models, listcat, measure = 'Fisher_discriminant')",
   "id": "b285485fd0d62677",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "max_rsa.plot_stats_one(compactness,models,  ['Categories', 'Normalized var'])",
   "id": "53a8a7fd78df0c80",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def sample_catrdm_pairs(cat_activations, submodels, n_samples=1000, nb_subcategories=12, nb_per_category = 50,\n",
    "                                    batch_size=10, seed=None):\n",
    "    \"\"\"\n",
    "    Memory-efficient version that processes in batches and optionally saves to disk.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    batch_size : int\n",
    "        Number of samples to process at once (default: 1000)\n",
    "    output_file : str, optional\n",
    "        If provided, saves results to this file using pickle\n",
    "    \"\"\"\n",
    "\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    dissimilarity_metric = 'L2squared'\n",
    "\n",
    "    nb_categories = len(cat_activations[submodels[0]])\n",
    "    n_batches = (n_samples + batch_size - 1) // batch_size\n",
    "\n",
    "    all_sims_samples = []\n",
    "    all_indices = []\n",
    "    print(f\"Processing {n_samples} samples in {n_batches} batches of {batch_size}...\")\n",
    "\n",
    "    batch_rdms = {}\n",
    "    for batch_idx in tqdm(range(n_batches)):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min(start_idx + batch_size, n_samples)\n",
    "        current_batch_size = end_idx - start_idx\n",
    "\n",
    "        subset_size = nb_subcategories\n",
    "        # Allocate batch arrays\n",
    "        batch_sim = np.zeros((current_batch_size))\n",
    "        batch_indices = np.zeros((current_batch_size, subset_size), dtype=int)\n",
    "        for model in submodels:\n",
    "            batch_rdms[model] = np.zeros((current_batch_size, nb_subcategories*nb_per_category, nb_subcategories*nb_per_category))\n",
    "        for i in range(current_batch_size):\n",
    "            # Randomly select images\n",
    "            cat_indices = np.random.choice(nb_categories, size=nb_subcategories, replace=False)\n",
    "\n",
    "            # Compute subrdms\n",
    "            for model in submodels:\n",
    "                batch_rdms[model][i] = rsa.compute_RDMs(cat_activations[model][cat_indices].reshape(nb_subcategories*nb_per_category, -1),\n",
    "                            metric=dissimilarity_metric, display=False)\n",
    "            # Extract submatrices\n",
    "            batch_sim[i] = rsa.Compute_sim_RDMs(batch_rdms[submodels[0]][i], batch_rdms[submodels[1]][i], center = False, metric = 'pearson' )\n",
    "            batch_indices[i] = cat_indices\n",
    "\n",
    "        all_sims_samples.append(batch_sim)\n",
    "        all_indices.append(batch_indices)\n",
    "\n",
    "    # Concatenate all batches\n",
    "    sim_samples = np.concatenate(all_sims_samples, axis=0)\n",
    "    indices_used = np.concatenate(all_indices, axis=0)\n",
    "\n",
    "\n",
    "    return sim_samples, indices_used\n",
    "\n",
    "\n"
   ],
   "id": "d224d33ccae3be52",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for i, model1 in enumerate(models[:-1]):\n",
    "    for j, model2 in enumerate(models[i+1:]):\n",
    "        sim_samples, indices_used = sample_catrdm_pairs(cat_activations, [model1, model2], n_samples=500, nb_subcategories = 12, nb_per_category = 50, batch_size=10, seed=None)\n",
    "        np.save(f'results/categories_sim_samples_{model1}_{model2}_{dataset}.npy', sim_samples)"
   ],
   "id": "ea6549f56c5e734",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a2d1c5a3dd794aa4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We have an idea of the average pearson similarity found if we select 12 categories - pretty high!\n",
    "Can we find a subset of 12 categories that has a much lover similarity than that. For example, categories with the lowest correlations between the models."
   ],
   "id": "69bd2416ca2ccd9a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def subsimilar_categories(cat_activations, submodels, dissimilarity_metric = 'L2squared', similarity_metric = 'pearson', nb_subcategories = 12):\n",
    "    assert len(submodels)== 2\n",
    "    assert cat_activations[submodels[0]].shape[:2] == cat_activations[submodels[1]].shape[:2]\n",
    "\n",
    "    shape = cat_activations[submodels[0]].shape\n",
    "\n",
    "    nb_categories = shape[0]\n",
    "    nb_per_categories = shape[1]\n",
    "\n",
    "    mean_cat_activations1 = cat_activations[submodels[0]].mean(axis = 1)\n",
    "    mean_cat_activations2 = cat_activations[submodels[1]].mean(axis = 1)\n",
    "\n",
    "    RDM1 = rsa.compute_RDMs(mean_cat_activations1,\n",
    "                            metric=dissimilarity_metric, display=False)\n",
    "    RDM2 = rsa.compute_RDMs(mean_cat_activations2,\n",
    "                            metric=dissimilarity_metric, display=False)\n",
    "\n",
    "    RDM1_centered = RDM1 - np.mean(RDM1)\n",
    "    RDM2_centered = RDM2 - np.mean(RDM2)\n",
    "\n",
    "    #RDM1_centered = RDM1_centered / np.sqrt(np.sum(RDM1_centered ** 2))\n",
    "    #RDM2_centered = RDM2_centered / np.sqrt(np.sum(RDM2_centered ** 2))\n",
    "\n",
    "    correlations = np.sum(RDM1_centered * RDM2_centered, axis=0)\n",
    "    subsimiliar_categories = np.argsort(correlations)[:nb_subcategories]\n",
    "\n",
    "    '''#subsimilar_RDM1 = rsa.compute_RDMs(cat_activations[submodels[0]][subsimiliar_categories].reshape(nb_subcategories*nb_per_categories, -1),\n",
    "                            metric=dissimilarity_metric, display=False)\n",
    "    #subsimilar_RDM2 = rsa.compute_RDMs(cat_activations[submodels[1]][subsimiliar_categories].reshape(nb_subcategories*nb_per_categories, -1),\n",
    "                            metric=dissimilarity_metric, display=False)\n",
    "    #print(rsa.Compute_sim_RDMs(subsimilar_RDM1, subsimilar_RDM2, metric = similarity_metric))'''\n",
    "    return correlations, subsimiliar_categories\n",
    "\n",
    "\n"
   ],
   "id": "ee0e5497fa21a2be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cat_similarities = {}\n",
    "similarities = {}\n",
    "for i, model1 in enumerate(models[:-1]):\n",
    "    for j, model2 in enumerate(models[i+1:]):\n",
    "        correlations, subsimilar_cats = subsimilar_categories(cat_activations, [model1, model2])\n",
    "        RDM1, RDM2, RDM1_sorted, RDM2_sorted, sorted_indices = max_rsa.find_subsimilar_subset(cat_activations, [model1, model2], subsimilar_cats,  images_per_subset = 4, nb_per_category = 50)\n",
    "        cat_sim = rsa.Compute_sim_RDMs(RDM1, RDM2, metric = 'pearson')\n",
    "        sim = rsa.Compute_sim_RDMs(RDM1_sorted, RDM2_sorted, metric = 'pearson')\n",
    "\n",
    "        '''fig, subs = plt.subplots(1,2, sharex=True, sharey=True)\n",
    "        subs[0].imshow(RDM1, cmap='gray')\n",
    "        subs[1].imshow(RDM2, cmap='gray')\n",
    "        subs[0].axis('off')\n",
    "        subs[1].axis('off')\n",
    "        fig.suptitle(f'{cat_sim}')\n",
    "        fig.tight_layout()'''\n",
    "        cat_similarities[f'{model1}_{model2}'] = cat_sim\n",
    "        print(subsimilar_cats)\n",
    "\n",
    "        fig, subs = plt.subplots(1,2, sharex=True, sharey=True)\n",
    "        subs[0].imshow(RDM1_sorted, cmap='gray')\n",
    "        subs[1].imshow(RDM2_sorted, cmap='gray')\n",
    "        subs[0].axis('off')\n",
    "        subs[1].axis('off')\n",
    "        fig.suptitle(f'{sim}')\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        similarities[f'{model1}_{model2}'] = sim"
   ],
   "id": "62f10091effdf3af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cat_similarities_compact = {}\n",
    "similarities_compact = {}\n",
    "sorted_indices = {}\n",
    "for i, model1 in enumerate(models[:-1]):\n",
    "    for j, model2 in enumerate(models[i+1:]):\n",
    "        nb_categories = len(listcat)\n",
    "        labels, sortedmaxdiffcats, maxdiffs = max_rsa.max_compactness_difference(\n",
    "                compact_categories, compactness, nb_categories, listcat, models = [model1, model2],\n",
    "                nb_considered_categories = 12, compactness_diff_measure = 'normalizedDiff'\n",
    "            )\n",
    "        RDM1, RDM2, RDM1_sorted, RDM2_sorted, sorted_indices[f'{model1}_{model2}'] = max_rsa.find_subsimilar_subset(cat_activations, [model1, model2], labels[:12],  images_per_subset = 4, nb_per_category = 50)\n",
    "        cat_sim = rsa.Compute_sim_RDMs(RDM1, RDM2, metric = 'pearson')\n",
    "        sim = rsa.Compute_sim_RDMs(RDM1_sorted, RDM2_sorted, metric = 'pearson')\n",
    "        '''fig, subs = plt.subplots(1,2, sharex=True, sharey=True)\n",
    "        subs[0].imshow(RDM1, cmap='gray')\n",
    "        subs[1].imshow(RDM2, cmap='gray')\n",
    "        subs[0].axis('off')\n",
    "        subs[1].axis('off')\n",
    "        fig.suptitle(f'{rsa.Compute_sim_RDMs(RDM1, RDM2, metric = 'pearson')}')\n",
    "        fig.tight_layout()'''\n",
    "        cat_similarities_compact[f'{model1}_{model2}'] = cat_sim\n",
    "\n",
    "        fig, subs = plt.subplots(1,2, sharex=True, sharey=True)\n",
    "        subs[0].imshow(RDM1_sorted, cmap='gray')\n",
    "        subs[1].imshow(RDM2_sorted, cmap='gray')\n",
    "        subs[0].axis('off')\n",
    "        subs[1].axis('off')\n",
    "        fig.suptitle(f'{rsa.Compute_sim_RDMs(RDM1_sorted, RDM2_sorted, metric = 'pearson')}')\n",
    "        fig.tight_layout()\n",
    "        similarities_compact[f'{model1}_{model2}'] = sim"
   ],
   "id": "d4b29f57ad3002ba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "dataset",
   "id": "5a2b3867dec2e99",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import glob\n",
    "listsamples = glob.glob(f'results/categories_sim_samples*{dataset}.npy')\n",
    "nb_cols = 5\n",
    "fig, subs = plt.subplots(nrows = 2, ncols = nb_cols, figsize = (25,10), sharex = True, sharey = True)\n",
    "for f, file in enumerate(listsamples):\n",
    "    sample = np.load(file)\n",
    "    hist, bin_edges = np.histogram(sample, 100)\n",
    "    subs[int(f/nb_cols), f%nb_cols].bar(bin_edges[:-1],hist/max(hist), width = bin_edges[1] - bin_edges[0], linewidth = 0, align = 'edge')\n",
    "    #subs[f//5, f%5].legend()\n",
    "    subs[int(f/nb_cols), f%nb_cols].set_xlabel('Similarity')\n",
    "    subs[int(f/nb_cols), f%nb_cols].set_ylabel('Density')\n",
    "    subs[int(f/nb_cols), f%nb_cols].set_title(f'{file.split('_')[-3]}_{file.split('_')[-2]}')\n",
    "    name = f'{file.split('_')[-3]}_{file.split('_')[-2]}'\n",
    "    subs[int(f/nb_cols), f%nb_cols].vlines(cat_similarities[name],0,1, 'g')\n",
    "    subs[int(f/nb_cols), f%nb_cols].vlines(cat_similarities_compact[name],0,1, 'r')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(f'figures/compactness/categories_selectionVSdistribution_{dataset}.png')\n",
    "plt.close()"
   ],
   "id": "3f8d54f348064cef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "listpickles_ecoLennyTest = glob.glob(f'/home/alban/Documents/results_image_selection/{dataset}_*.pkl')\n",
    "\n",
    "RESULTS = {}\n",
    "for p, pkl in enumerate(listpickles_ecoLennyTest):\n",
    "    name = pkl.split('/')[-1][:-4]\n",
    "    f = open(pkl, \"rb\")\n",
    "    RESULTS[name] = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "list_names = [k for k in RESULTS.keys()]\n",
    "similarities_compactness = {}\n",
    "\n",
    "similarities_compactness = [RESULTS[f'{dataset}_Truenormalize_silhouette_score_normalizedDiff_pearson']['similarity_dict'][pair]['similarity'] for pair in\n",
    "                      RESULTS[f'{dataset}_Truenormalize_silhouette_score_normalizedDiff_pearson']['similarity_dict'].keys()]"
   ],
   "id": "c9042710785b5c2b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import glob\n",
    "\n",
    "nb_cols = 5\n",
    "fig, subs = plt.subplots(nrows = 2, ncols = nb_cols, figsize = (25,10), sharex = True, sharey = True)\n",
    "f = 0\n",
    "for i, model1 in enumerate(models[:-1]):\n",
    "    for j, model2 in enumerate(models[i+1:]):\n",
    "        sample = np.load(f'results/sim_samples_{model1}_{model2}_{dataset}.npy')\n",
    "        hist, bin_edges = np.histogram(sample, 100)\n",
    "        subs[int(f/nb_cols), f%nb_cols].bar(bin_edges[:-1],hist/max(hist), width = bin_edges[1] - bin_edges[0], linewidth = 0, align = 'edge')\n",
    "        #subs[f//5, f%5].legend()\n",
    "        subs[int(f/nb_cols), f%nb_cols].set_xlabel('Similarity')\n",
    "        subs[int(f/nb_cols), f%nb_cols].set_ylabel('Density')\n",
    "        subs[int(f/nb_cols), f%nb_cols].set_title(f'{model1}_{model2}')\n",
    "        subs[int(f/nb_cols), f%nb_cols].vlines(similarities[f'{model1}_{model2}'],0,1, 'green')\n",
    "        subs[int(f/nb_cols), f%nb_cols].vlines(similarities_compactness[f],0,1, 'r')\n",
    "        subs[int(f/nb_cols), f%nb_cols].vlines(similarities_compact[f'{model1}_{model2}'],0,1, 'y')\n",
    "        #subs[int(f/nb_cols), f%nb_cols].vlines(sample.mean(),0,1, 'k')\n",
    "        subs[int(f/nb_cols), f%nb_cols].set_xlim(-0.21,0.9)\n",
    "        f +=1\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(f'figures/compactness/selectionVSdistribution_{dataset}.png')\n",
    "plt.close()"
   ],
   "id": "22aa38bd33757249",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "### Comparing the RDMs for the 2 dataset\n",
    "random_categories = [50,89,260,478,12,205,401,256,369,78]\n",
    "submodels = ['saycam', 'imagenet']\n",
    "for dataset in ['ecoVal', 'ecoLennyTest']:\n",
    "    models  = ['ego', 'saycam', 'imagenet', 'supervised', 'resnet']\n",
    "    #models  = ['ego', 'saycam']\n",
    "    path2activations = f'/home/alban/Documents/activations_datadriven/%s_{dataset}/'\n",
    "\n",
    "    imagelists = {}\n",
    "    activations = {}\n",
    "    for model in models:\n",
    "        with open(join(path2activations%model, 'imagepaths.txt'), 'r') as f:\n",
    "            imagelists[model] = [line.strip() for line in f.readlines()]\n",
    "        activations[model] = np.load(join(path2activations % model, 'cls_tokens.npy'))\n",
    "\n",
    "    activations[model].shape\n",
    "    #### Normalize vectors\n",
    "    for model in models:\n",
    "        norms = np.linalg.norm(activations[model], axis=1, keepdims=True)\n",
    "        activations[model] = activations[model]/norms # normalization\n",
    "\n",
    "    ### reshape activations according to include categories\n",
    "    cat_activations = activations.copy()\n",
    "    for model in models:\n",
    "        shape = activations[model].shape\n",
    "        cat_activations[model] = activations[model].reshape(-1, nb_per_cat, shape[-1])\n",
    "\n",
    "    shape = activations[model].shape\n",
    "    cat_activations[model] = activations[model].reshape(-1, nb_per_cat, shape[-1])\n",
    "\n",
    "    cat_activations_subset1 = cat_activations[submodels[0]][random_categories]\n",
    "    cat_activations_subset2 = cat_activations[submodels[1]][random_categories]\n",
    "\n",
    "    cat_shape = cat_activations_subset1.shape\n",
    "\n",
    "    RDM1 = rsa.compute_RDMs(cat_activations_subset1.reshape(cat_shape[0] * cat_shape[1], -1),\n",
    "                            metric='L2squared', display=False)\n",
    "    RDM2 = rsa.compute_RDMs(cat_activations_subset2.reshape(cat_shape[0] * cat_shape[1], -1),\n",
    "                            metric='L2squared', display=False)\n",
    "\n",
    "    fig, subs = plt.subplots(1,2, sharex=True, sharey=True)\n",
    "    subs[0].imshow(RDM1, cmap='gray')\n",
    "    subs[1].imshow(RDM2, cmap='gray')\n",
    "    subs[0].axis('off')\n",
    "    subs[1].axis('off')\n",
    "    fig.suptitle(f'{rsa.Compute_sim_RDMs(RDM1, RDM2, metric = 'pearson')}')\n",
    "    fig.tight_layout()"
   ],
   "id": "d7c68aef0af3e8ea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "## Looking at selections\n",
    "#imagelist = [img.replace('/raid/leonard_vandyck/datasets/genloc/', '/home/alban/Documents/ecoLennyTest/') for img in imagelist]\n",
    "imagelist = [img.replace('/raid/shared/datasets/visoin/ecoset/', '/home/alban/Documents/ecoset/') for img in imagelist]\n",
    "imagespaths = {}\n",
    "for i, model1 in enumerate(models[:-1]):\n",
    "    for j, model2 in enumerate(models[i+1:]):\n",
    "        savename = f'Truenormalize_Fisher_discriminant_corr_{model1}_{model2}_ecoTest'\n",
    "        images, imagespaths[model1 + '_' + model2] = max_rsa.display_low_similarity_images(imagelist, sorted_indices[f'{model1}_{model2}'], n_images=48,\n",
    "                                                      grid_cols=8, figsize=(20, 12),\n",
    "                                                      save_path=f'figures/compactness/subset/{savename}.png')"
   ],
   "id": "a91c14712e2a3163",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "imagelist",
   "id": "a20776914397fdb5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "RDMs = {}\n",
    "metric = 'L2squared'\n",
    "for i, model in enumerate(models):\n",
    "    print(model)\n",
    "    RDMs[model] = rsa.compute_RDMs(activations[model], metric = metric, display = False, title = f'{model}_{metric}')"
   ],
   "id": "8d081628f8724375",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mean_RDM = RDMs.copy()\n",
    "for model in models:\n",
    "    mean_RDM[model] = RDMs[model].reshape(len(listcat), nb_per_cat, len(listcat), nb_per_cat)\n",
    "    mean_RDM[model] = mean_RDM[model].transpose(0, 2, 1, 3)\n",
    "    mean_RDM[model] = mean_RDM[model].mean(axis = (2,3))\n",
    "\n",
    "fig, subs = plt.subplots(1,2, sharex=True, sharey=True)\n",
    "subs[0].imshow(mean_RDM[submodels[0]], cmap='gray')\n",
    "subs[1].imshow(mean_RDM[submodels[1]], cmap='gray')\n",
    "subs[0].axis('off')\n",
    "subs[1].axis('off')\n",
    "fig.suptitle(f'{rsa.Compute_sim_RDMs(mean_RDM[submodels[0]], mean_RDM[submodels[1]], metric = 'pearson')}')\n",
    "fig.tight_layout()"
   ],
   "id": "1373d2404e38a291",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "submodels = ['saycam', 'ego']\n",
    "nb_categories = len(listcat)\n",
    "labels, sortedmaxdiffcats, maxdiffs = max_rsa.max_compactness_difference(\n",
    "                compact_categories, compactness, nb_categories, listcat, models = submodels,\n",
    "                nb_considered_categories = 12, compactness_diff_measure = 'normalizedDiff'\n",
    "            )"
   ],
   "id": "6efab2def0aa5252",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e434fb53f677e306",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "9635db2521f11e7d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from itertools import combinations\n",
    "categories = labels[:12]\n",
    "dissimilarity_metric = 'L2squared'\n",
    "images_per_subset = 4\n",
    "\n",
    "#### First build the RDMs using all images of the chosen categories to get the general stats\n",
    "cat_activations_subset1 = cat_activations[submodels[0]][categories]\n",
    "cat_activations_subset2 = cat_activations[submodels[1]][categories]\n",
    "\n",
    "cat_shape = cat_activations_subset1.shape\n",
    "\n",
    "RDM1 = rsa.compute_RDMs(cat_activations_subset1.reshape(cat_shape[0]*cat_shape[1], -1), metric = dissimilarity_metric, display = False)\n",
    "RDM2 = rsa.compute_RDMs(cat_activations_subset2.reshape(cat_shape[0] * cat_shape[1], -1),\n",
    "                        metric=dissimilarity_metric, display=False)\n",
    "means = {}\n",
    "n = len(RDM1)\n",
    "upper_indices = np.triu_indices(n, k=1)  # k=1 excludes diagonal\n",
    "means['x'] = np.mean(RDM1[upper_indices])\n",
    "means['y'] = np.mean(RDM2[upper_indices])\n",
    "means['norm'] = np.std(RDM1[upper_indices]) * np.std(RDM2[upper_indices])\n",
    "print(means)\n",
    "for c, category in enumerate(tqdm(categories[:2], desc=\"Processing categories\")):\n",
    "    print(f\"\\nProcessing category: {category}\")\n",
    "    # Get activations for both models for this category\n",
    "    cat_RDM1 = RDM1[c*nb_per_cat:(c+1)*nb_per_cat, c*nb_per_cat:(c+1)*nb_per_cat]  # Shape: (50, 50)\n",
    "    cat_RDM2 = RDM2[c*nb_per_cat:(c+1)*nb_per_cat, c*nb_per_cat:(c+1)*nb_per_cat]  # Shape: (50, 50)\n",
    "\n",
    "    # Generate combinations of image indices\n",
    "    all_combinations = list(combinations(range(nb_per_cat), images_per_subset))\n",
    "\n",
    "    print(f\"Testing {len(all_combinations)} combinations of {images_per_subset} images\")\n",
    "\n",
    "    best_indices = None\n",
    "    best_model1_rdm = None\n",
    "    best_model2_rdm = None\n",
    "    best_similarity = np.inf\n",
    "\n",
    "    # Test each combination\n",
    "    for combination in tqdm(all_combinations, desc=\"Testing combinations\", leave=False, position=1):\n",
    "        indices = np.array(combination)\n",
    "        # Get subset of activations\n",
    "        rdm1 = cat_RDM1[np.ix_(indices, indices)]  # Shape: (4, 4)\n",
    "        rdm2 = cat_RDM2[np.ix_(indices, indices)]  # Shape: (4, 4)\n",
    "\n",
    "        # Compute similarity between RDMs\n",
    "        similarity = rsa.Compute_sim_RDMs(rdm1, rdm2, center = False, metric = 'pearson_global', means= means)\n",
    "\n",
    "        # Update best if this is better\n",
    "        if similarity < best_similarity:\n",
    "            best_indices = indices\n",
    "            best_model1_rdm = rdm1\n",
    "            best_model2_rdm = rdm2\n",
    "            best_similarity = similarity\n",
    "\n",
    "    print(f\"Best indices for {category}: {best_indices}\")\n",
    "    print(f\"Similarity: {best_similarity:.4f}\")\n"
   ],
   "id": "a2474ed3df2b73b2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "np.corrcoef(RDM1.flatten(), RDM2.flatten()).shape",
   "id": "17e1da56ec78c21b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "max_dissimilarity_images = max_rsa.find_max_dissimilarity_images(\n",
    "                cat_activations, submodels, labels[:12], nb_per_cat,\n",
    "                images_per_subset=4, similarity_metric='pearson', diff = maxdiffs\n",
    "            )\n",
    "similarity_dict = max_rsa.compute_sub_rdm_similarity(\n",
    "            max_dissimilarity_images, cat_activations, submodels, labels[:12],\n",
    "            savename = '')"
   ],
   "id": "d6a47ac5c850ae53",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def sample_rdm_pairs(RDM1, RDM2, n_samples=100000, subset_size=40,\n",
    "                                    batch_size=10000, seed=None):\n",
    "    \"\"\"\n",
    "    Memory-efficient version that processes in batches and optionally saves to disk.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    batch_size : int\n",
    "        Number of samples to process at once (default: 1000)\n",
    "    output_file : str, optional\n",
    "        If provided, saves results to this file using pickle\n",
    "    \"\"\"\n",
    "\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    n_images = RDM1.shape[0]\n",
    "    n_batches = (n_samples + batch_size - 1) // batch_size\n",
    "\n",
    "    all_sims_samples = []\n",
    "    all_indices = []\n",
    "    print(f\"Processing {n_samples} samples in {n_batches} batches of {batch_size}...\")\n",
    "\n",
    "    for batch_idx in tqdm_notebook(range(n_batches)):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min(start_idx + batch_size, n_samples)\n",
    "        current_batch_size = end_idx - start_idx\n",
    "\n",
    "        # Allocate batch arrays\n",
    "        batch_sim = np.zeros((current_batch_size))\n",
    "        batch_indices = np.zeros((current_batch_size, subset_size), dtype=int)\n",
    "\n",
    "        for i in range(current_batch_size):\n",
    "            # Randomly select images\n",
    "            indices = np.random.choice(n_images, size=subset_size, replace=False)\n",
    "            indices = np.sort(indices)\n",
    "\n",
    "            # Extract submatrices\n",
    "            batch_sim[i] = rsa.Compute_sim_RDMs(RDM1[np.ix_(indices, indices)], RDM2[np.ix_(indices, indices)], center = False, metric = 'pearson' )\n",
    "            batch_indices[i] = indices\n",
    "\n",
    "        all_sims_samples.append(batch_sim)\n",
    "        all_indices.append(batch_indices)\n",
    "\n",
    "    # Concatenate all batches\n",
    "    sim_samples = np.concatenate(all_sims_samples, axis=0)\n",
    "    indices_used = np.concatenate(all_indices, axis=0)\n",
    "\n",
    "\n",
    "    return sim_samples, indices_used"
   ],
   "id": "49fa7525fb08d37b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from itertools import combinations\n",
    "def find_max_dissimilarity_images(cat_activations, models, categories, nb_per_cat,\n",
    "                                  images_per_subset=4, dissimilarity_metric = 'L2squared', diff = np.array([0])):\n",
    "    \"\"\"\n",
    "    Find the subset of images per category that maximizes RDM dissimilarity between two models.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    cat_activations : dict\n",
    "        Dictionary with structure: cat_activations[model][category] = array of activations (n_images, n_features)\n",
    "    models : list\n",
    "        List of two model names, e.g., ['model1', 'model2']\n",
    "    categories : list\n",
    "        List of category names/indices\n",
    "    compute_RDM : function\n",
    "        Function that takes activations and returns RDM: RDM = compute_RDM(activations)\n",
    "    images_per_subset : int\n",
    "        Number of images to select per category (default: 4)\n",
    "    method : str\n",
    "        'exhaustive' or 'random' sampling of combinations\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    results : dict\n",
    "        Dictionary with results for each category:\n",
    "        {\n",
    "            category: {\n",
    "                'best_indices': array of selected image indices,\n",
    "                'max_dissimilarity': maximum dissimilarity value,\n",
    "                'model1_rdm': RDM for model1 with selected images,\n",
    "                'model2_rdm': RDM for model2 with selected images,\n",
    "                'similarity': similarity between the two RDMs\n",
    "            }\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    if len(models) != 2:\n",
    "        raise ValueError(\"This function requires exactly 2 models\")\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    #### First build the RDMs using all images of the chosen categories to get the general stats\n",
    "    cat_activations_subset1 = cat_activations[models[0]][categories]\n",
    "    cat_activations_subset2 = cat_activations[models[1]][categories]\n",
    "\n",
    "    cat_shape = cat_activations_subset1.shape\n",
    "\n",
    "    RDM1 = rsa.compute_RDMs(cat_activations_subset1.reshape(cat_shape[0]*cat_shape[1], -1), metric = dissimilarity_metric, display = False)\n",
    "    RDM2 = rsa.compute_RDMs(cat_activations_subset2.reshape(cat_shape[0] * cat_shape[1], -1),\n",
    "                            metric=dissimilarity_metric, display=False)\n",
    "    means = {}\n",
    "    n = len(RDM1)\n",
    "    upper_indices = np.triu_indices(n, k=1)  # k=1 excludes diagonal\n",
    "    means['x'] = np.mean(RDM1[upper_indices])\n",
    "    means['y'] = np.mean(RDM2[upper_indices])\n",
    "    means['norm'] = np.std(RDM1[upper_indices]) * np.std(RDM2[upper_indices])\n",
    "    print(means)\n",
    "    for c, category in enumerate(tqdm_notebook(categories, desc=\"Processing categories\")):\n",
    "        print(f\"\\nProcessing category: {category}\")\n",
    "        # Get activations for both models for this category\n",
    "        cat_RDM1 = RDM1[c*nb_per_cat:(c+1)*nb_per_cat, c*nb_per_cat:(c+1)*nb_per_cat]  # Shape: (50, 50)\n",
    "        cat_RDM2 = RDM2[c*nb_per_cat:(c+1)*nb_per_cat, c*nb_per_cat:(c+1)*nb_per_cat]  # Shape: (50, 50)\n",
    "\n",
    "        # Generate combinations of image indices\n",
    "        all_combinations = list(combinations(range(nb_per_cat), images_per_subset))\n",
    "\n",
    "        print(f\"Testing {len(all_combinations)} combinations of {images_per_subset} images\")\n",
    "\n",
    "        best_indices = None\n",
    "        best_model1_rdm = None\n",
    "        best_model2_rdm = None\n",
    "        best_similarity = 1\n",
    "\n",
    "        # Test each combination\n",
    "        for combination in tqdm_notebook(all_combinations, desc=\"Testing combinations\", leave=False, position=1):\n",
    "            indices = np.array(combination)\n",
    "            # Get subset of activations\n",
    "            rdm1 = cat_RDM1[np.ix_(indices, indices)]  # Shape: (4, 4)\n",
    "            rdm2 = cat_RDM2[np.ix_(indices, indices)]  # Shape: (4, 4)\n",
    "\n",
    "            n = len(rdm1)\n",
    "            upper_indices = np.triu_indices(n, k=1)  # k=1 excludes diagonal\n",
    "            if diff[c] <0:\n",
    "                similarity = -np.mean(rdm1[upper_indices]) + np.mean(rdm2[upper_indices]) +  2*np.std(rdm1[upper_indices]) + np.std(rdm2[upper_indices])\n",
    "            else:\n",
    "                similarity = np.mean(rdm1[upper_indices]) - np.mean(rdm2[upper_indices]) +  2*np.std(rdm1[upper_indices]) + np.std(rdm2[upper_indices])\n",
    "\n",
    "            # Update best if this is better\n",
    "            if similarity < best_similarity:\n",
    "                best_indices = indices\n",
    "                best_model1_rdm = rdm1\n",
    "                best_model2_rdm = rdm2\n",
    "                best_similarity = similarity\n",
    "\n",
    "        # Store results for this category\n",
    "        results[category] = {\n",
    "            'best_indices': best_indices,\n",
    "            'model1_rdm': best_model1_rdm,\n",
    "            'model2_rdm': best_model2_rdm,\n",
    "            'similarity': best_similarity\n",
    "        }\n",
    "\n",
    "        print(f\"Best indices for {category}: {best_indices}\")\n",
    "        print(f\"Similarity: {best_similarity:.4f}\")\n",
    "\n",
    "    return results"
   ],
   "id": "bd7697374011838d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "labels, sortedmaxdiffcats, maxdiffs = max_rsa.max_compactness_difference(\n",
    "                compact_categories, compactness, 565, listcat, models = ['saycam', 'supervised'],\n",
    "                nb_considered_categories = 12, compactness_diff_measure = 'normalizedDiff'\n",
    "            )\n",
    "max_dissimilarity_images = find_max_dissimilarity_images(\n",
    "                cat_activations, ['saycam', 'supervised'], labels[:12], 50,\n",
    "                images_per_subset=4, diff = maxdiffs\n",
    "            )\n",
    "similarity_dict = max_rsa.compute_sub_rdm_similarity(\n",
    "            max_dissimilarity_images, cat_activations, ['saycam', 'supervised'], labels[:12],\n",
    "            savename = '')"
   ],
   "id": "955746ffd7427021",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "fe97521aca52f9dd",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
