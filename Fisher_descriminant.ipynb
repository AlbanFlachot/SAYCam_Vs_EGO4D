{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-10T15:21:10.749032Z",
     "start_time": "2025-09-10T15:21:08.816659Z"
    }
   },
   "source": [
    "import importlib\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from matplotlib import pyplot as plt\n",
    "from os.path import join\n",
    "import os\n",
    "import seaborn as sns\n",
    "from torchvision.ops.misc import interpolate\n",
    "from tqdm.notebook import tqdm as tqdm_notebook\n",
    "\n",
    "#### Custum libraries\n",
    "import lib.algos_maxRSA as max_rsa\n",
    "import lib.utils_RSA as rsa\n",
    "import lib.utils_CKA as cka\n",
    "from lib.algos import *\n",
    "from scipy.spatial import procrustes as scipro\n",
    "\n",
    "\n",
    "importlib.reload(rsa)\n",
    "importlib.reload(cka)\n",
    "importlib.reload(max_rsa)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'lib.algos_maxRSA' from '/home/alban/projects/SAYCam_Vs_EGO4D/lib/algos_maxRSA.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T15:21:11.301582Z",
     "start_time": "2025-09-10T15:21:10.809030Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = 'ecoVal'\n",
    "models  = ['ego', 'saycam', 'imagenet', 'supervised', 'random', 'resnet']\n",
    "#models  = ['ego', 'saycam']\n",
    "path2activations = f'/home/alban/Documents/activations_datadriven/%s_{dataset}/'\n",
    "\n",
    "imagelists = {}\n",
    "activations = {}\n",
    "for model in models:\n",
    "    with open(join(path2activations%model, 'imagepaths.txt'), 'r') as f:\n",
    "        imagelists[model] = [line.strip() for line in f.readlines()]\n",
    "    activations[model] = np.load(join(path2activations % model, 'cls_tokens.npy'))\n",
    "\n",
    "activations[model].shape"
   ],
   "id": "9d74a01f946de792",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28250, 2048)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T15:21:11.476775Z",
     "start_time": "2025-09-10T15:21:11.316644Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#### Normalize vectors\n",
    "for model in models:\n",
    "    norms = np.linalg.norm(activations[model], axis=1, keepdims=True)\n",
    "    activations[model] = activations[model]/norms # normalization"
   ],
   "id": "57b4b8f165f71803",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T15:22:23.248257Z",
     "start_time": "2025-09-10T15:21:11.491941Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#### Compute RDMs performing squared euclidean distance as a metric (to be equivalent to CKA methods, cf. Williams, 2024)\n",
    "RDMs = {}\n",
    "submodels = ['ego', 'saycam', 'imagenet', 'supervised', 'resnet']\n",
    "metric = 'L2squared'\n",
    "for i, model in enumerate(submodels):\n",
    "    print(model)\n",
    "    RDMs[model] = rsa.compute_RDMs(activations[model], metric = metric, display = False, title = f'{model}_{metric}')\n",
    "    #RDMs[model] = cka.centering(RDMs[model]) # same as centering with matmul\n"
   ],
   "id": "20a5beb160dafd1e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ego\n",
      "saycam\n",
      "imagenet\n",
      "supervised\n",
      "resnet\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T15:22:23.409915Z",
     "start_time": "2025-09-10T15:22:23.390583Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### check if images were shown in the same order\n",
    "assert imagelists['ego'] == imagelists['saycam']\n",
    "imagelist = imagelists['ego'] # since they are the same, only consider one list\n",
    "\n",
    "#### check if each category has the same number of images and list all categories in listcats\n",
    "count = 0\n",
    "cat = ''\n",
    "listcat = list()\n",
    "for i, imgp in enumerate(imagelist):\n",
    "    current_cat = imgp.split('/')[-2]\n",
    "    if i == 0:\n",
    "        cat = current_cat\n",
    "        listcat.append(current_cat)\n",
    "    if cat != current_cat:\n",
    "        cat = current_cat\n",
    "        listcat.append(current_cat)\n",
    "        count = 1\n",
    "    else:\n",
    "        count += 1\n",
    "\n",
    "nb_per_cat = count # in val, 50 images per cate\n",
    "\n",
    "nb_per_cat\n",
    "#np.array(imagelist).reshape(-1, nb_per_cat)[0]"
   ],
   "id": "d4235475cff135a6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T15:22:23.470410Z",
     "start_time": "2025-09-10T15:22:23.467130Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### reshape activations according to include categories\n",
    "cat_activations = activations.copy()\n",
    "\n",
    "for model in models:\n",
    "    shape = activations[model].shape\n",
    "    cat_activations[model] = activations[model].reshape(-1, nb_per_cat, shape[-1])"
   ],
   "id": "7b03ab76b544c7db",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T15:22:23.643720Z",
     "start_time": "2025-09-10T15:22:23.519714Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Compute representational compactness for each category and model\n",
    "compactness, compact_categories = max_rsa.compute_Fisher_descriminant(cat_activations, models, listcat)\n"
   ],
   "id": "47c1722da20051e5",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'lib.algos_maxRSA' has no attribute 'compute_Fisher_descriminant'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAttributeError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m### Compute representational compactness for each category and model\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m compactness, compact_categories = \u001B[43mmax_rsa\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcompute_Fisher_descriminant\u001B[49m(cat_activations, models, listcat)\n",
      "\u001B[31mAttributeError\u001B[39m: module 'lib.algos_maxRSA' has no attribute 'compute_Fisher_descriminant'"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "max_rsa.plot_stats_one(compactness,models,  ['Categories', 'Normalized var'], savename='Fisher_descriminant_allmodels.png')\n",
    "for model in models:\n",
    "    print(f'{model}')\n",
    "    print(compact_categories[model][:20])\n",
    "for model in models:\n",
    "    print(compact_categories[model][-20:])\n"
   ],
   "id": "ea33a0b5d1286755",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def check_list_similarity(list1, list2):\n",
    "    '''Checks if two lists contain the same elements, regardless of order,\n",
    "    and calculates the proportion of common elements.'''\n",
    "    set1 = set(list1)\n",
    "    set2 = set(list2)\n",
    "    common_elements = set1 & set2  # Intersection of sets\n",
    "    proportion = (len(common_elements) / max(len(set1), len(set2))) * 100 if max(len(set1), len(set2)) > 0 else 0\n",
    "    return proportion\n",
    "\n",
    "\n",
    "for model in models:\n",
    "    print(f'\\n Overlap {model} and the others')\n",
    "    for model2 in models[:]:\n",
    "        print(check_list_similarity(compact_categories[model][:50],compact_categories[model2][:50]))\n"
   ],
   "id": "aed86411af350dee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "### Compute 500 random sequence for significance\n",
    "# initialize sequences\n",
    "nb_trials = 500\n",
    "idx_vec = np.array(range(len(listcat)))\n",
    "mat_vec = np.zeros((nb_trials, len(idx_vec)))\n",
    "for i in range(nb_trials):\n",
    "    np.random.shuffle(idx_vec)\n",
    "    mat_vec[i] = idx_vec\n",
    "\n",
    "#Compute all possible similarity pairs\n",
    "list_sim = []\n",
    "for i in range(len(mat_vec)-1):\n",
    "    for j in range(i+1,len(mat_vec)):\n",
    "        list_sim.append(check_list_similarity(list(mat_vec[i][:50]), list(mat_vec[j][:50])))\n",
    "\n",
    "# Compute 95 percentile\n",
    "confinter = np.percentile(list_sim, 95)\n",
    "print(f'The 95% confidence interval is:{confinter}')"
   ],
   "id": "6ce9c8abfb1b1c34",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "nb_categories = len(listcat)\n",
    "labels, sortedmaxdiffcats, maxdiffs = max_rsa.max_compactness_difference(compact_categories, compactness, nb_categories, listcat, models = ['ego', 'saycam'], nb_max_compactness = 20)"
   ],
   "id": "889c85cf93c3912a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from itertools import combinations\n",
    "def find_max_dissimilarity_images(RDMs, models, categories, nb_per_cat,\n",
    "                                  images_per_subset=4):\n",
    "    \"\"\"\n",
    "    Find the subset of images per category that maximizes RDM dissimilarity between two models.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    cat_activations : dict\n",
    "        Dictionary with structure: cat_activations[model][category] = array of activations (n_images, n_features)\n",
    "    models : list\n",
    "        List of two model names, e.g., ['model1', 'model2']\n",
    "    categories : list\n",
    "        List of category names/indices\n",
    "    compute_RDM : function\n",
    "        Function that takes activations and returns RDM: RDM = compute_RDM(activations)\n",
    "    images_per_subset : int\n",
    "        Number of images to select per category (default: 4)\n",
    "    method : str\n",
    "        'exhaustive' or 'random' sampling of combinations\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    results : dict\n",
    "        Dictionary with results for each category:\n",
    "        {\n",
    "            category: {\n",
    "                'best_indices': array of selected image indices,\n",
    "                'max_dissimilarity': maximum dissimilarity value,\n",
    "                'model1_rdm': RDM for model1 with selected images,\n",
    "                'model2_rdm': RDM for model2 with selected images,\n",
    "                'similarity': similarity between the two RDMs\n",
    "            }\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    if len(models) != 2:\n",
    "        raise ValueError(\"This function requires exactly 2 models\")\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    means = {}\n",
    "    n = len(RDMs[models[0]])\n",
    "    upper_indices = np.triu_indices(n, k=1)  # k=1 excludes diagonal\n",
    "    means['x'] = np.mean(RDMs[models[0]][upper_indices])\n",
    "    means['y'] = np.mean(RDMs[models[1]][upper_indices])\n",
    "    means['norm'] = np.std(RDMs[models[0]][upper_indices]) * np.std(RDMs[models[1]][upper_indices])\n",
    "    print(means)\n",
    "    for category in tqdm_notebook(categories, desc=\"Processing categories\"):\n",
    "        print(f\"\\nProcessing category: {category}\")\n",
    "        # Get activations for both models for this category\n",
    "        RDM1 = RDMs[models[0]][category*nb_per_cat:(category+1)*nb_per_cat, category*nb_per_cat:(category+1)*nb_per_cat]  # Shape: (50, 50)\n",
    "        RDM2 = RDMs[models[1]][category*nb_per_cat:(category+1)*nb_per_cat, category*nb_per_cat:(category+1)*nb_per_cat]  # Shape: (50, 50)\n",
    "\n",
    "        # Generate combinations of image indices\n",
    "        all_combinations = list(combinations(range(nb_per_cat), images_per_subset))\n",
    "\n",
    "        print(f\"Testing {len(all_combinations)} combinations of {images_per_subset} images\")\n",
    "\n",
    "        best_indices = None\n",
    "        best_model1_rdm = None\n",
    "        best_model2_rdm = None\n",
    "        best_similarity = 1\n",
    "\n",
    "        # Test each combination\n",
    "        for combination in tqdm_notebook(all_combinations, desc=\"Testing combinations\", leave=False, position=1):\n",
    "            indices = np.array(combination)\n",
    "            # Get subset of activations\n",
    "            rdm1 = RDM1[np.ix_(indices, indices)]  # Shape: (4, 4)\n",
    "            rdm2 = RDM2[np.ix_(indices, indices)]  # Shape: (4, 4)\n",
    "\n",
    "            # Compute similarity between RDMs\n",
    "\n",
    "            similarity = rsa.Compute_sim_RDMs(rdm1, rdm2, center = False, metric = 'pearson_global', means= means)\n",
    "\n",
    "            # Update best if this is better\n",
    "            if similarity < best_similarity:\n",
    "                best_indices = indices\n",
    "                best_model1_rdm = rdm1\n",
    "                best_model2_rdm = rdm2\n",
    "                best_similarity = similarity\n",
    "\n",
    "        # Store results for this category\n",
    "        results[category] = {\n",
    "            'best_indices': best_indices,\n",
    "            'model1_rdm': best_model1_rdm,\n",
    "            'model2_rdm': best_model2_rdm,\n",
    "            'similarity': best_similarity\n",
    "        }\n",
    "\n",
    "        print(f\"Best indices for {category}: {best_indices}\")\n",
    "        print(f\"Similarity: {best_similarity:.4f}\")\n",
    "\n",
    "    return results\n",
    "\n"
   ],
   "id": "cd61c75b86bab403",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "results = find_max_dissimilarity_images(\n",
    "        RDMs, ['ego', 'saycam'], labels[:10], 50,\n",
    "        images_per_subset=4\n",
    "    )"
   ],
   "id": "4d540a8a37799fc4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def compute_sub_rdm_similarity(results, RDMs, models, categories):\n",
    "    \"\"\"\n",
    "    Compute sub-RDMs using the 40 selected images (4 per category × 10 categories)\n",
    "    that maximize dissimilarity between models, then compute their similarity.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    results : dict\n",
    "        Output from find_max_dissimilarity_images() with structure:\n",
    "        results[category]['best_indices'] = array of 4 selected image indices\n",
    "    full_rdms : dict\n",
    "        Dictionary: full_rdms[model] = full RDM array (25000, 25000)\n",
    "    models : list\n",
    "        List of two model names, e.g., ['model1', 'model2']\n",
    "    categories : list\n",
    "        List of category names/indices (should have 10 categories)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    result : dict\n",
    "        Dictionary containing:\n",
    "        {\n",
    "            'similarity': similarity between the two 40×40 RDMs,\n",
    "            'model1_rdm': 40×40 RDM for model1,\n",
    "            'model2_rdm': 40×40 RDM for model2,\n",
    "            'image_info': list of (category, original_index) for each of the 40 images\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    if len(models) != 2:\n",
    "        raise ValueError(\"This function requires exactly 2 models\")\n",
    "\n",
    "    print(f\"Collecting 40 selected images from {len(categories)} categories...\")\n",
    "\n",
    "    means = {}\n",
    "    n = len(RDMs[models[0]])\n",
    "    upper_indices = np.triu_indices(n, k=1)  # k=1 excludes diagonal\n",
    "    means['x'] = np.mean(RDMs[models[0]][upper_indices])\n",
    "    means['y'] = np.mean(RDMs[models[1]][upper_indices])\n",
    "    means['norm'] = np.std(RDMs[models[0]][upper_indices]) * np.std(RDMs[models[1]][upper_indices])\n",
    "    # Collect selected image indices from all categories\n",
    "    selected_indices = []\n",
    "    image_info = []  # Track which category and original index each image comes from\n",
    "\n",
    "    total_selected = 0\n",
    "\n",
    "    for category in categories:\n",
    "        if category not in results:\n",
    "            raise ValueError(f\"Category {category} not found in results\")\n",
    "\n",
    "        # Get the 4 selected indices for this category\n",
    "        cat_selected_indices = results[category]['best_indices']\n",
    "\n",
    "        if len(cat_selected_indices) != 4:\n",
    "            print(f\"Warning: Category {category} has {len(cat_selected_indices)} images instead of 4\")\n",
    "\n",
    "        # Add to combined list\n",
    "        selected_indices.extend(cat_selected_indices + 50*category)\n",
    "\n",
    "        # Track image information\n",
    "        for idx in cat_selected_indices:\n",
    "            image_info.append((category, idx))\n",
    "\n",
    "        total_selected += len(cat_selected_indices)\n",
    "        #print(f\"Category {category}: selected indices {cat_selected_indices}\")\n",
    "\n",
    "    print(f\"\\nTotal selected images: {total_selected}\")\n",
    "\n",
    "    # Verify we have 40 images\n",
    "    if len(selected_indices) != 40:\n",
    "        print(f\"Warning: Expected 40 images, got {len(selected_indices)}\")\n",
    "\n",
    "    # Extract sub-RDMs for both models using the 40 selected images\n",
    "    print(\"Extracting sub-RDMs...\")\n",
    "    rdm_model1 = RDMs[models[0]][np.ix_(selected_indices , selected_indices )]  # Shape: (40, 40)\n",
    "    rdm_model2 = RDMs[models[1]][np.ix_(selected_indices , selected_indices  )]  # Shape: (40, 40)\n",
    "\n",
    "    print(f\"RDM shapes: {rdm_model1.shape}, {rdm_model2.shape}\")\n",
    "\n",
    "    # Compute similarity between the two RDMs\n",
    "    print(\"Computing similarity between RDMs...\")\n",
    "    similarity = rsa.Compute_sim_RDMs(rdm_model1, rdm_model2, center = False, metric = 'pearson')\n",
    "\n",
    "    print(f\"\\nRDM similarity using 40 maximally dissimilar images: {similarity:.6f}\")\n",
    "\n",
    "    # Package results\n",
    "    result = {\n",
    "        'similarity': similarity,\n",
    "        'model1_rdm': rdm_model1,\n",
    "        'model2_rdm': rdm_model2,\n",
    "        'image_info': image_info,\n",
    "        'total_images': total_selected,\n",
    "        'selected_indices': selected_indices\n",
    "    }\n",
    "\n",
    "    fig, subs = plt.subplots(1,2)\n",
    "    sns.heatmap(rdm_model1,\n",
    "                annot=False,\n",
    "                cmap='Greys',      # Blue to red colormap\n",
    "                square=True,\n",
    "                cbar=True,\n",
    "                #cbar_kws={'label': 'Dissimilarity'},\n",
    "                #fmt='.2f',\n",
    "                linewidths=0,\n",
    "                ax = subs[0],\n",
    "                vmin=0,               # Set minimum value for color scale\n",
    "                vmax=np.max(rdm_model1))               # Set maximum value for color scale\n",
    "    sns.heatmap(rdm_model2,\n",
    "                annot=False,\n",
    "                cmap='Greys',      # Blue to red colormap\n",
    "                square=True,\n",
    "                cbar=True,\n",
    "                #cbar_kws={'label': 'Dissimilarity'},\n",
    "                #fmt='.2f',\n",
    "                linewidths=0,\n",
    "                ax = subs[1],\n",
    "                vmin=0,               # Set minimum value for color scale\n",
    "                vmax=np.max(rdm_model2))\n",
    "\n",
    "\n",
    "    subs[0].axis('off')\n",
    "    subs[1].axis('off')\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    return result"
   ],
   "id": "b140ef3152b9a903",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "similarity_dict = compute_sub_rdm_similarity(results, RDMs, ['ego', 'saycam'], labels[:10])\n",
   "id": "2e335f90b2386692",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "def sample_rdm_pairs(RDM1, RDM2, n_samples=100000, subset_size=40,\n",
    "                                    batch_size=10000, seed=None):\n",
    "    \"\"\"\n",
    "    Memory-efficient version that processes in batches and optionally saves to disk.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    batch_size : int\n",
    "        Number of samples to process at once (default: 1000)\n",
    "    output_file : str, optional\n",
    "        If provided, saves results to this file using pickle\n",
    "    \"\"\"\n",
    "\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    n_images = RDM1.shape[0]\n",
    "    n_batches = (n_samples + batch_size - 1) // batch_size\n",
    "\n",
    "    all_sims_samples = []\n",
    "    all_indices = []\n",
    "    means = {}\n",
    "    n = len(RDMs[models[0]])\n",
    "    upper_indices = np.triu_indices(n, k=1)  # k=1 excludes diagonal\n",
    "    means['x'] = np.mean(RDM1[upper_indices])\n",
    "    means['y'] = np.mean(RDM2[upper_indices])\n",
    "    means['norm'] = np.std(RDM1[upper_indices]) * np.std(RDM2[upper_indices])\n",
    "    print(f\"Processing {n_samples} samples in {n_batches} batches of {batch_size}...\")\n",
    "\n",
    "    for batch_idx in tqdm_notebook(range(n_batches)):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min(start_idx + batch_size, n_samples)\n",
    "        current_batch_size = end_idx - start_idx\n",
    "\n",
    "        # Allocate batch arrays\n",
    "        batch_sim = np.zeros((current_batch_size))\n",
    "        batch_indices = np.zeros((current_batch_size, subset_size), dtype=int)\n",
    "\n",
    "        for i in range(current_batch_size):\n",
    "            # Randomly select images\n",
    "            indices = np.random.choice(n_images, size=subset_size, replace=False)\n",
    "            indices = np.sort(indices)\n",
    "\n",
    "            # Extract submatrices\n",
    "            batch_sim[i] = rsa.Compute_sim_RDMs(RDM1[np.ix_(indices, indices)], RDM2[np.ix_(indices, indices)], center = False, metric = 'pearson' )\n",
    "            batch_indices[i] = indices\n",
    "\n",
    "        all_sims_samples.append(batch_sim)\n",
    "        all_indices.append(batch_indices)\n",
    "\n",
    "    # Concatenate all batches\n",
    "    sim_samples = np.concatenate(all_sims_samples, axis=0)\n",
    "    indices_used = np.concatenate(all_indices, axis=0)\n",
    "\n",
    "\n",
    "    return sim_samples, indices_used\n",
    "\n",
    "\n"
   ],
   "id": "5573fae9148cb27d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sim_samples, indices_used = sample_rdm_pairs(RDMs['ego'], RDMs['saycam'], n_samples=1000000, subset_size=40,\n",
    "                                    batch_size=1000, seed=None)"
   ],
   "id": "add2ef64a559802d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "hist, bin_edges = np.histogram(sim_samples, 100)\n",
    "plt.bar(bin_edges[:-1],hist/max(hist), width = bin_edges[1] - bin_edges[0], align = 'edge')\n",
    "plt.vlines(similarity_dict['similarity'], 0, 1, 'green', label = 'Subset_selected')\n",
    "plt.vlines(np.mean(sim_samples), 0, 1, 'r', label = 'Average')\n",
    "plt.legend()\n",
    "plt.xlabel('Similarity')\n",
    "plt.ylabel('Density')\n",
    "plt.show()\n",
    "perc = np.sum(sim_samples< similarity_dict['similarity'])/len(sim_samples)\n"
   ],
   "id": "9f49ab9e561d3bd2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "perc*len(sim_samples)",
   "id": "5d8590729d82be3d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "def display_low_similarity_images(image_paths, indices_vectorized, n_images=40,\n",
    "                                 grid_cols=8, figsize=(20, 10), save_path=None):\n",
    "    \"\"\"\n",
    "    Load and display the first n images corresponding to lowest similarity indices.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    image_paths : list\n",
    "        List of image file paths matching the RDM column indices\n",
    "    indices_vectorized : numpy.ndarray\n",
    "        Column indices sorted from lowest to highest similarity\n",
    "    n_images : int\n",
    "        Number of images to display (default: 40)\n",
    "    grid_cols : int\n",
    "        Number of columns in the display grid (default: 8)\n",
    "    figsize : tuple\n",
    "        Figure size for matplotlib (default: (20, 10))\n",
    "    save_path : str, optional\n",
    "        Path to save the figure (if None, just display)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    loaded_images : list\n",
    "        List of loaded images (as numpy arrays)\n",
    "    valid_paths : list\n",
    "        List of valid image paths that were successfully loaded\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the indices for the first n_images with lowest similarity\n",
    "    low_similarity_indices = indices_vectorized[:n_images]\n",
    "\n",
    "    # Get corresponding image paths\n",
    "    selected_paths = [image_paths[idx] for idx in low_similarity_indices]\n",
    "\n",
    "    loaded_images = []\n",
    "    valid_paths = []\n",
    "    valid_indices = []\n",
    "\n",
    "    print(f\"Loading {n_images} images with lowest RDM column similarity...\")\n",
    "\n",
    "    # Load images\n",
    "    for i, (path, orig_idx) in enumerate(zip(selected_paths, low_similarity_indices)):\n",
    "        try:\n",
    "            # Check if file exists\n",
    "            if not os.path.exists(path):\n",
    "                print(f\"Warning: File not found: {path}\")\n",
    "                continue\n",
    "\n",
    "            # Load image with cv2\n",
    "            img = cv2.imread(path)\n",
    "\n",
    "            if img is None:\n",
    "                print(f\"Warning: Could not load image: {path}\")\n",
    "                continue\n",
    "\n",
    "            # Convert BGR to RGB for matplotlib display\n",
    "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            loaded_images.append(img_rgb)\n",
    "            valid_paths.append(path)\n",
    "            valid_indices.append(orig_idx)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {path}: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"Successfully loaded {len(loaded_images)} out of {n_images} requested images\")\n",
    "\n",
    "    if len(loaded_images) == 0:\n",
    "        print(\"No images could be loaded!\")\n",
    "        return [], []\n",
    "\n",
    "    # Calculate grid dimensions\n",
    "    n_loaded = len(loaded_images)\n",
    "    grid_rows = (n_loaded + grid_cols - 1) // grid_cols\n",
    "\n",
    "    # Create figure and display images\n",
    "    fig, axes = plt.subplots(grid_rows, grid_cols, figsize=figsize)\n",
    "    fig.suptitle(f'Images leading to lowest similarity',\n",
    "                 fontsize=16, y=0.98)\n",
    "\n",
    "    # Handle case where we have only one row\n",
    "    if grid_rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "\n",
    "    for i in range(grid_rows * grid_cols):\n",
    "        row = i // grid_cols\n",
    "        col = i % grid_cols\n",
    "        ax = axes[row, col]\n",
    "\n",
    "        if i < len(loaded_images):\n",
    "            # Display image\n",
    "            ax.imshow(loaded_images[i])\n",
    "\n",
    "            # Add title with original index and filename\n",
    "            filename = Path(valid_paths[i]).name\n",
    "            label = valid_paths[i].split('/')[-2]\n",
    "            ax.set_title(f'Label: {label }\\n{filename[:20]}...',\n",
    "                        fontsize=8, pad=2)\n",
    "\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save or show\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"Figure saved to: {save_path}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return loaded_images, valid_paths"
   ],
   "id": "dc8be8d4a3d1d5db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "imagelist = [img.replace('/raid/shared/datasets/visoin/', '/home/alban/Documents/') for img in imagelist]\n",
    "images, paths = display_low_similarity_images(imagelist, similarity_dict['selected_indices'], n_images=40,\n",
    "                                 grid_cols=8, figsize=(20, 10), save_path='figures/images_inverse_Fisher_descriminant.png')"
   ],
   "id": "166a408f21f1a38a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's do the same for the same analysis for the models trained on saycam and imagenet.\n",
   "id": "3cc8132ddf39fa98"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "labels_say_imagenet, sortedmaxdiffcats_say_imagenet, maxdiffs_say_imagenet = max_rsa.max_compactness_difference(compact_categories, compactness, nb_categories, listcat, models = ['saycam', 'imagenet'], nb_max_compactness = 20)\n",
    "\n",
    "results_say_imagenet = find_max_dissimilarity_images(\n",
    "        RDMs, ['saycam', 'imagenet'], labels_say_imagenet[:10], 50,\n",
    "        images_per_subset=4)\n",
    "\n"
   ],
   "id": "c058e60a78a089ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "similarity_dict_say_imagenet = compute_sub_rdm_similarity(results_say_imagenet, RDMs, ['saycam', 'imagenet'], labels_say_imagenet[:10])\n",
    "imagelist = [img.replace('/raid/shared/datasets/visoin/', '/home/alban/Documents/') for img in imagelist]\n",
    "images, paths = display_low_similarity_images(imagelist, similarity_dict_say_imagenet['selected_indices'], n_images=40,\n",
    "                                 grid_cols=8, figsize=(20, 10), save_path='figures/images_inverse_Fisher_descriminant_saycam_imagnet.png')"
   ],
   "id": "e518b3d30c9a2e13",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sim_samples_say_imagenet, indices_used_say_imagenet = sample_rdm_pairs(RDMs['saycam'], RDMs['imagenet'], n_samples=1000000, subset_size=40, batch_size=1000, seed=None)\n",
   "id": "27e23cc6ac20061d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "hist, bin_edges = np.histogram(sim_samples_say_imagenet, 100)\n",
    "plt.bar(bin_edges[:-1],hist/max(hist), width = bin_edges[1] - bin_edges[0], align = 'edge')\n",
    "plt.vlines(similarity_dict_say_imagenet['similarity'], 0, 1, 'green', label = 'Subset_selected')\n",
    "plt.vlines(np.mean(sim_samples_say_imagenet), 0, 1, 'r', label = 'Average')\n",
    "plt.legend()\n",
    "plt.xlabel('Similarity')\n",
    "plt.ylabel('Density')\n",
    "plt.show()\n",
    "perc = np.sum(sim_samples_say_imagenet< similarity_dict_say_imagenet['similarity'])/len(sim_samples_say_imagenet)"
   ],
   "id": "98a4a2077cceeeb8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "perc*len(sim_samples_say_imagenet)",
   "id": "fe46a4b19112cbf0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Same thing between saycam and the supervised.",
   "id": "68f7fdf55081aa8f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "labels_say_supervised, sortedmaxdiffcats_say_supervised, maxdiffs_say_supervised = max_rsa.max_compactness_difference(compact_categories, compactness, nb_categories, listcat, models = ['saycam', 'supervised'], nb_max_compactness = 20)\n",
    "\n",
    "results_say_supervised = find_max_dissimilarity_images(\n",
    "        RDMs, ['saycam', 'supervised'], labels_say_supervised[:10], 50,\n",
    "        images_per_subset=4)\n",
    "\n"
   ],
   "id": "b4c20472cd726d6c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "9fcce15834c2723e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "similarity_dict_say_supervised = compute_sub_rdm_similarity(results_say_supervised, RDMs, ['saycam', 'supervised'], labels_say_supervised[:10])\n",
    "imagelist = [img.replace('/raid/shared/datasets/visoin/', '/home/alban/Documents/') for img in imagelist]\n",
    "images, paths = display_low_similarity_images(imagelist, similarity_dict_say_supervised['selected_indices'], n_images=40,\n",
    "                                 grid_cols=8, figsize=(20, 10), save_path='figures/images_inverse_Fisher_descriminant_saycam_supervised.png')"
   ],
   "id": "c2c88e23054e124c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sim_samples_say_supervised, indices_used_say_supervised = sample_rdm_pairs(RDMs['saycam'], RDMs['supervised'], n_samples=1000000, subset_size=40, batch_size=1000, seed=None)",
   "id": "5c87aadb242ff660",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "hist, bin_edges = np.histogram(sim_samples_say_supervised, 100)\n",
    "plt.bar(bin_edges[:-1],hist/max(hist), width = bin_edges[1] - bin_edges[0], align = 'edge')\n",
    "plt.vlines(similarity_dict_say_supervised['similarity'], 0, 1, 'green', label = 'Subset_selected')\n",
    "plt.vlines(np.mean(sim_samples_say_supervised), 0, 1, 'r', label = 'Average')\n",
    "plt.legend()\n",
    "plt.xlabel('Similarity')\n",
    "plt.ylabel('Density')\n",
    "plt.show()\n",
    "perc = np.sum(sim_samples_say_supervised< similarity_dict_say_supervised['similarity'])/len(sim_samples_say_supervised)"
   ],
   "id": "3a5faa589bb69ad1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "labels_say_resnet, sortedmaxdiffcats_say_resnet, maxdiffs_say_resnet = max_rsa.max_compactness_difference(compact_categories, compactness, nb_categories, listcat, models = ['saycam', 'resnet'], nb_max_compactness = 20)\n",
    "\n",
    "results_say_resnet = find_max_dissimilarity_images(\n",
    "        RDMs, ['saycam', 'resnet'], labels_say_resnet[:10], 50,\n",
    "        images_per_subset=4)"
   ],
   "id": "26024c8fcb25ab6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "similarity_dict_say_resnet = compute_sub_rdm_similarity(results_say_resnet, RDMs, ['saycam', 'resnet'], labels_say_resnet[:10])\n",
    "imagelist = [img.replace('/raid/shared/datasets/visoin/', '/home/alban/Documents/') for img in imagelist]\n",
    "images, paths = display_low_similarity_images(imagelist, similarity_dict_say_resnet['selected_indices'], n_images=40,\n",
    "                                 grid_cols=8, figsize=(20, 10), save_path='figures/images_inverse_Fisher_descriminant_saycam_resnet.png')"
   ],
   "id": "7b40922d8ea58e4a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "nb_categories = len(listcat)\n",
    "labels_say_resnet, sortedmaxdiffcats_say_resnet, maxdiffs_say_resnet = max_rsa.max_compactness_difference(compact_categories, compactness, nb_categories, listcat, models = ['saycam', 'resnet'], nb_max_compactness = 20)\n",
    "\n",
    "results_say_resnet = find_max_dissimilarity_images(\n",
    "        RDMs, ['saycam', 'resnet'], labels_say_resnet[:10], 50,\n",
    "        images_per_subset=4)"
   ],
   "id": "37d351606800a505",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "similarity_dict_say_resnet = compute_sub_rdm_similarity(results_say_resnet, RDMs, ['saycam', 'resnet'], labels_say_resnet[:10])\n",
    "imagelist = [img.replace('/raid/shared/datasets/visoin/', '/home/alban/Documents/') for img in imagelist]\n",
    "images, paths = display_low_similarity_images(imagelist, similarity_dict_say_resnet['selected_indices'], n_images=40,\n",
    "                                 grid_cols=8, figsize=(20, 10), save_path='figures/images_inverse_Fisher_descriminant_saycam_resnet.png')"
   ],
   "id": "16290a09547bdfb4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sim_samples_say_resnet, indices_used_say_resnet = sample_rdm_pairs(RDMs['saycam'], RDMs['resnet'], n_samples=1000000, subset_size=40, batch_size=1000, seed=None)",
   "id": "c685904d1b7822b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "hist, bin_edges = np.histogram(sim_samples_say_resnet, 100)\n",
    "plt.bar(bin_edges[:-1],hist/max(hist), width = bin_edges[1] - bin_edges[0], align = 'edge')\n",
    "plt.vlines(similarity_dict_say_resnet['similarity'], 0, 1, 'green', label = 'Subset_selected')\n",
    "plt.vlines(np.mean(sim_samples_say_resnet), 0, 1, 'r', label = 'Average')\n",
    "plt.legend()\n",
    "plt.xlabel('Similarity')\n",
    "plt.ylabel('Density')\n",
    "plt.show()\n",
    "perc = np.sum(sim_samples_say_resnet< similarity_dict_say_resnet['similarity'])/len(sim_samples_say_resnet)\n",
    "print(f'Number of Similarity samples with a lower similarity than our method: {int(perc*len(sim_samples))}')"
   ],
   "id": "1ab5c77c973d186e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
