{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import importlib\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from matplotlib import pyplot as plt\n",
    "from os.path import join\n",
    "import os\n",
    "import seaborn as sns\n",
    "from torchvision.ops.misc import interpolate\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "#### Custum libraries\n",
    "import lib.algos_maxRSA as max_rsa\n",
    "import lib.utils_RSA as rsa\n",
    "import lib.utils_CKA as cka\n",
    "from lib.algos import *\n",
    "\n",
    "\n",
    "importlib.reload(rsa)\n",
    "importlib.reload(cka)\n",
    "importlib.reload(max_rsa)"
   ],
   "id": "93e9f31bd96905b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dataset = 'ecoLennyTest'\n",
    "arch = 'vgg'\n",
    "models  = ['faces', 'dual', 'objects', 'random']\n",
    "#models  = ['ego', 'saycam']\n",
    "path2activations = f'/home/alban/Documents/activations_datadriven/{arch}%s_{dataset}/'\n",
    "\n",
    "imagelists = {}\n",
    "activations = {}\n",
    "for model in models:\n",
    "    with open(join(path2activations%model, 'imagepaths.txt'), 'r') as f:\n",
    "        imagelists[model] = [line.strip() for line in f.readlines()]\n",
    "    activations[model] = np.load(join(path2activations % model, 'fc1_outputs.npy'))\n",
    "\n",
    "imagelist = imagelists[model]\n",
    "activations[model].shape"
   ],
   "id": "dda873529bcfd1a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#### Normalize vectors\n",
    "for model in models:\n",
    "    norms = np.linalg.norm(activations[model], axis=1, keepdims=True)\n",
    "    activations[model] = activations[model]/norms # normalization"
   ],
   "id": "98c15462363182e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "### check if images were shown in the same order\n",
    "assert imagelists[models[0]] == imagelists[models[1]]\n",
    "imagelist = imagelists[models[0]] # since they are the same, only consider one list\n",
    "\n",
    "#### check if each category has the same number of images and list all categories in listcats\n",
    "count = 0\n",
    "cat = ''\n",
    "listcat = list()\n",
    "for i, imgp in enumerate(imagelist):\n",
    "    current_cat = imgp.split('/')[-2]\n",
    "    if i == 0:\n",
    "        cat = current_cat\n",
    "        listcat.append(current_cat)\n",
    "    if cat != current_cat:\n",
    "        cat = current_cat\n",
    "        listcat.append(current_cat)\n",
    "        count = 1\n",
    "    else:\n",
    "        count += 1\n",
    "\n",
    "nb_per_cat = count # in val, 50 images per cate\n",
    "\n",
    "nb_per_cat"
   ],
   "id": "204242663570ea70",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "### reshape activations according to include categories\n",
    "cat_activations = activations.copy()\n",
    "\n",
    "for model in models:\n",
    "    shape = activations[model].shape\n",
    "    cat_activations[model] = activations[model].reshape(-1, nb_per_cat, shape[-1])"
   ],
   "id": "991ad3c111deb76",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "savedir = 'results/compactness/fisher_discriminant_vggDual/'\n",
    "if not os.path.exists(savedir):\n",
    "    os.makedirs(savedir)\n",
    "    sorted_compactness, sorted_compact_categories, compactness = max_rsa.compute_compactness(cat_activations, models, listcat, measure = 'Fisher_discriminant')\n",
    "    np.save(join(savedir, 'compactness.npy'), compactness)\n",
    "    np.save(join(savedir, 'sorted_compactness.npy'), sorted_compactness)\n",
    "    np.save(join(savedir, 'sorted_compact_categories.npy'), sorted_compact_categories)\n",
    "else: # if we computed and saved compactness before, let's just load it\n",
    "    compactness = np.load(join(savedir, 'compactness.npy'), allow_pickle=True).item()\n",
    "    sorted_compactness = np.load(join(savedir, 'sorted_compactness.npy'), allow_pickle=True).item()\n",
    "    sorted_compact_categories = np.load(join(savedir, 'sorted_compact_categories.npy'), allow_pickle=True).item()"
   ],
   "id": "6ed4b0bb9c990681",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "fig_compactness, ax_compactness = max_rsa.plot_stats_one(sorted_compactness,models,  ['Categories', 'Normalized var'])",
   "id": "53a8a7fd78df0c80",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def check_list_similarity(list1, list2):\n",
    "    '''Checks if two lists contain the same elements, regardless of order,\n",
    "    and calculates the proportion of common elements.'''\n",
    "    set1 = set(list1)\n",
    "    set2 = set(list2)\n",
    "    common_elements = set1 & set2  # Intersection of sets\n",
    "    proportion = (len(common_elements) / max(len(set1), len(set2))) * 100 if max(len(set1), len(set2)) > 0 else 0\n",
    "    return proportion\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "model_overlap_matrix= np.zeros((len(models), len(models)))\n",
    "\n",
    "for m1, model1 in enumerate(models):\n",
    "    for m2, model2 in enumerate(models):\n",
    "        model_overlap_matrix[m1,m2] = check_list_similarity(sorted_compact_categories[model1][:50],sorted_compact_categories[model2][:50])\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.rcParams['axes.grid'] = False\n",
    "# Replace the plt.imshow() section with:\n",
    "sns.heatmap(model_overlap_matrix,\n",
    "        annot=True, fmt='.1f', cmap='grey',\n",
    "        xticklabels=models, yticklabels=models, cbar=False)\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=45)\n",
    "#plt.imshow(model_overlap_matrix[dataset], cmap = 'grey')\n",
    "#plt.xticks(np.arange(len(models)), models, rotation = 45)\n",
    "#plt.yticks(np.arange(len(models)), models, rotation = 45)\n",
    "plt.show()"
   ],
   "id": "f7b5f2688e15d72b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def sample_catrdm_pairs(cat_activations, submodels, n_samples=1000, nb_subcategories=12, nb_per_category = 50,\n",
    "                                    batch_size=10, seed=None):\n",
    "    \"\"\"\n",
    "    Memory-efficient version that processes in batches and optionally saves to disk.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    batch_size : int\n",
    "        Number of samples to process at once (default: 1000)\n",
    "    output_file : str, optional\n",
    "        If provided, saves results to this file using pickle\n",
    "    \"\"\"\n",
    "\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    dissimilarity_metric = 'L2squared'\n",
    "\n",
    "    nb_categories = len(cat_activations[submodels[0]])\n",
    "    n_batches = (n_samples + batch_size - 1) // batch_size\n",
    "\n",
    "    all_sims_samples = []\n",
    "    all_indices = []\n",
    "    print(f\"Processing {n_samples} samples in {n_batches} batches of {batch_size}...\")\n",
    "\n",
    "    batch_rdms = {}\n",
    "    for batch_idx in tqdm(range(n_batches)):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min(start_idx + batch_size, n_samples)\n",
    "        current_batch_size = end_idx - start_idx\n",
    "\n",
    "        subset_size = nb_subcategories\n",
    "        # Allocate batch arrays\n",
    "        batch_sim = np.zeros((current_batch_size))\n",
    "        batch_indices = np.zeros((current_batch_size, subset_size), dtype=int)\n",
    "        for model in submodels:\n",
    "            batch_rdms[model] = np.zeros((current_batch_size, nb_subcategories*nb_per_category, nb_subcategories*nb_per_category))\n",
    "        for i in range(current_batch_size):\n",
    "            # Randomly select images\n",
    "            cat_indices = np.random.choice(nb_categories, size=nb_subcategories, replace=False)\n",
    "\n",
    "            # Compute subrdms\n",
    "            for model in submodels:\n",
    "                batch_rdms[model][i] = rsa.compute_RDMs(cat_activations[model][cat_indices].reshape(nb_subcategories*nb_per_category, -1),\n",
    "                            metric=dissimilarity_metric, display=False)\n",
    "            # Extract submatrices\n",
    "            batch_sim[i] = rsa.Compute_sim_RDMs(batch_rdms[submodels[0]][i], batch_rdms[submodels[1]][i], center = False, metric = 'pearson' )\n",
    "            batch_indices[i] = cat_indices\n",
    "\n",
    "        all_sims_samples.append(batch_sim)\n",
    "        all_indices.append(batch_indices)\n",
    "\n",
    "    # Concatenate all batches\n",
    "    sim_samples = np.concatenate(all_sims_samples, axis=0)\n",
    "    indices_used = np.concatenate(all_indices, axis=0)\n",
    "\n",
    "\n",
    "    return sim_samples, indices_used\n",
    "\n",
    "\n"
   ],
   "id": "d224d33ccae3be52",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for i, model1 in enumerate(models[:-2]):\n",
    "    for j, model2 in enumerate(models[i+1:-1]): # ignore random\n",
    "        sim_samples, indices_used = sample_catrdm_pairs(cat_activations, [model1, model2], n_samples=50, nb_subcategories = 12, nb_per_category = 50, batch_size=10, seed=None)\n",
    "        np.save(f'results/categories_sim_samples_{arch}_{model1}_{model2}_{dataset}.npy', sim_samples)"
   ],
   "id": "ea6549f56c5e734",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a2d1c5a3dd794aa4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We have an idea of the average pearson similarity found if we select 12 categories - pretty high!\n",
    "Can we find a subset of 12 categories that has a much lover similarity than that. For example, categories with the lowest correlations between the models."
   ],
   "id": "69bd2416ca2ccd9a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def subsimilar_categories(cat_activations, submodels, dissimilarity_metric = 'L2squared', similarity_metric = 'pearson', nb_subcategories = 12):\n",
    "    assert len(submodels)== 2\n",
    "    assert cat_activations[submodels[0]].shape[:2] == cat_activations[submodels[1]].shape[:2]\n",
    "\n",
    "    shape = cat_activations[submodels[0]].shape\n",
    "\n",
    "    nb_categories = shape[0]\n",
    "    nb_per_categories = shape[1]\n",
    "\n",
    "    mean_cat_activations1 = cat_activations[submodels[0]].mean(axis = 1)\n",
    "    mean_cat_activations2 = cat_activations[submodels[1]].mean(axis = 1)\n",
    "\n",
    "    RDM1 = rsa.compute_RDMs(mean_cat_activations1,\n",
    "                            metric=dissimilarity_metric, display=False)\n",
    "    RDM2 = rsa.compute_RDMs(mean_cat_activations2,\n",
    "                            metric=dissimilarity_metric, display=False)\n",
    "\n",
    "    RDM1_centered = RDM1 - np.mean(RDM1)\n",
    "    RDM2_centered = RDM2 - np.mean(RDM2)\n",
    "\n",
    "    #RDM1_centered = RDM1_centered / np.sqrt(np.sum(RDM1_centered ** 2))\n",
    "    #RDM2_centered = RDM2_centered / np.sqrt(np.sum(RDM2_centered ** 2))\n",
    "\n",
    "    correlations = np.sum(RDM1_centered * RDM2_centered, axis=0)\n",
    "    subsimiliar_categories = np.argsort(correlations)[:nb_subcategories]\n",
    "\n",
    "    '''#subsimilar_RDM1 = rsa.compute_RDMs(cat_activations[submodels[0]][subsimiliar_categories].reshape(nb_subcategories*nb_per_categories, -1),\n",
    "                            metric=dissimilarity_metric, display=False)\n",
    "    #subsimilar_RDM2 = rsa.compute_RDMs(cat_activations[submodels[1]][subsimiliar_categories].reshape(nb_subcategories*nb_per_categories, -1),\n",
    "                            metric=dissimilarity_metric, display=False)\n",
    "    #print(rsa.Compute_sim_RDMs(subsimilar_RDM1, subsimilar_RDM2, metric = similarity_metric))'''\n",
    "    return correlations, subsimiliar_categories\n",
    "\n",
    "\n"
   ],
   "id": "ee0e5497fa21a2be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cat_similarities = {}\n",
    "similarities = {}\n",
    "for i, model1 in enumerate(models[:-1]):\n",
    "    for j, model2 in enumerate(models[i+1:]):\n",
    "        correlations, subsimilar_cats = subsimilar_categories(cat_activations, [model1, model2], nb_subcategories = 12)\n",
    "        RDM1, RDM2, RDM1_sorted, RDM2_sorted, sorted_indices = max_rsa.find_subsimilar_subset(cat_activations, [model1, model2], subsimilar_cats,  images_per_subset = 4, nb_per_category = 50)\n",
    "        cat_sim = rsa.Compute_sim_RDMs(RDM1, RDM2, metric = 'pearson')\n",
    "        sim = rsa.Compute_sim_RDMs(RDM1_sorted, RDM2_sorted, metric = 'pearson')\n",
    "\n",
    "        '''fig, subs = plt.subplots(1,2, sharex=True, sharey=True)\n",
    "        subs[0].imshow(RDM1, cmap='gray')\n",
    "        subs[1].imshow(RDM2, cmap='gray')\n",
    "        subs[0].axis('off')\n",
    "        subs[1].axis('off')\n",
    "        fig.suptitle(f'{cat_sim}')\n",
    "        fig.tight_layout()'''\n",
    "        cat_similarities[f'{model1}_{model2}'] = cat_sim\n",
    "        print(np.array(listcat)[subsimilar_cats])\n",
    "\n",
    "        fig, subs = plt.subplots(1,2, sharex=True, sharey=True)\n",
    "        subs[0].imshow(RDM1_sorted, cmap='gray')\n",
    "        subs[1].imshow(RDM2_sorted, cmap='gray')\n",
    "        subs[0].axis('off')\n",
    "        subs[1].axis('off')\n",
    "        fig.suptitle(f'{sim}')\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        similarities[f'{model1}_{model2}'] = sim"
   ],
   "id": "62f10091effdf3af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "nb_seleted_categories = 12\n",
    "cat_similarities_compact = {}\n",
    "similarities_compact = {}\n",
    "sorted_indices = {}\n",
    "maxdiffs = {}\n",
    "for i, model1 in enumerate(models[:-1]):\n",
    "    for j, model2 in enumerate(models[i+1:]):\n",
    "        labels, sortedmaxdiffcats, maxdiffs[f'{model1}_{model2}'] = max_rsa.max_compactness_difference(\n",
    "                sorted_compact_categories, compactness, listcat, models = [model1, model2],\n",
    "                nb_considered_categories = nb_seleted_categories, compactness_diff_measure = 'normalizedDiff'\n",
    "            )\n",
    "        RDM1, RDM2, RDM1_sorted, RDM2_sorted, sorted_indices[f'{model1}_{model2}'] = max_rsa.find_subsimilar_subset(cat_activations, [model1, model2], labels[:nb_seleted_categories],  images_per_subset = 4, nb_per_category = 50)\n",
    "        cat_sim = rsa.Compute_sim_RDMs(RDM1, RDM2, metric = 'pearson')\n",
    "        sim = rsa.Compute_sim_RDMs(RDM1_sorted, RDM2_sorted, metric = 'pearson')\n",
    "        '''fig, subs = plt.subplots(1,2, sharex=True, sharey=True)\n",
    "        subs[0].imshow(RDM1, cmap='gray')\n",
    "        subs[1].imshow(RDM2, cmap='gray')\n",
    "        subs[0].axis('off')\n",
    "        subs[1].axis('off')\n",
    "        fig.suptitle(f'{rsa.Compute_sim_RDMs(RDM1, RDM2, metric = 'pearson')}')\n",
    "        fig.tight_layout()'''\n",
    "        cat_similarities_compact[f'{model1}_{model2}'] = cat_sim\n",
    "\n",
    "        savename = f'Truenormalize_Fisher_discriminant_corr_{arch}_{model1}_{model2}_ecoLennyTest'\n",
    "        fig, subs = plt.subplots(1,2, sharex=True, sharey=True)\n",
    "        subs[0].imshow(RDM1_sorted, cmap='gray')\n",
    "        subs[1].imshow(RDM2_sorted, cmap='gray')\n",
    "        subs[0].axis('off')\n",
    "        subs[1].axis('off')\n",
    "        fig.suptitle(f'{rsa.Compute_sim_RDMs(RDM1_sorted, RDM2_sorted, metric = 'pearson')}')\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(f'figures/compactness/subRDMs_vgg/{savename}.png')\n",
    "        similarities_compact[f'{model1}_{model2}'] = sim\n"
   ],
   "id": "d4b29f57ad3002ba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import glob\n",
    "listsamples = glob.glob(f'results/categories_sim_samples_{arch}_*_{dataset}.npy')\n",
    "nb_cols = 3\n",
    "fig, subs = plt.subplots(nrows = 1, ncols = nb_cols, figsize = (25,10), sharex = True, sharey = True)\n",
    "for f, file in enumerate(listsamples):\n",
    "    sample = np.load(file)\n",
    "    hist, bin_edges = np.histogram(sample, 100)\n",
    "    subs[f].bar(bin_edges[:-1],hist/max(hist), width = bin_edges[1] - bin_edges[0], linewidth = 0, align = 'edge')\n",
    "    #subs[f//5, f%5].legend()\n",
    "    subs[f].set_xlabel('Similarity')\n",
    "    subs[f].set_ylabel('Density')\n",
    "    subs[f].set_title(f'{file.split('_')[-3]}_{file.split('_')[-2]}')\n",
    "    name = f'{file.split('_')[-3]}_{file.split('_')[-2]}'\n",
    "    subs[f].vlines(cat_similarities[name],0,1, 'g')\n",
    "    subs[f].vlines(cat_similarities_compact[name],0,1, 'r')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "#fig.savefig(f'figures/compactness/categories_sim_samples_{arch}_{model1}_{model2}_{dataset}.npy')\n",
    "plt.close()"
   ],
   "id": "58f66277920d4d9f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(list(sorted_compact_categories['objects']).index('0001_man'))\n",
    "print(list(sorted_compact_categories['faces']).index('0001_man'))\n"
   ],
   "id": "d64b5b099e8e25ba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "list(sortedmaxdiffcats).index('0001_man')",
   "id": "5a2b3867dec2e99",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "3//4",
   "id": "7c6ad5a95387c432",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "## Looking at selections\n",
    "imagelist = [img.replace('/raid/leonard_vandyck/datasets/genloc/', '/home/alban/Documents/ecoLennyTest/') for img in imagelist]\n",
    "#imagelist = [img.replace('/raid/shared/datasets/visoin/ecoset/', '/home/alban/Documents/ecoset/') for img in imagelist]\n",
    "imagespaths = {}\n",
    "for i, model1 in enumerate(models[:-1]):\n",
    "    for j, model2 in enumerate(models[i+1:]):\n",
    "        savename = f'Truenormalize_Fisher_discriminant_corr_{arch}_{model1}_{model2}_ecoLennyTest'\n",
    "        images, imagespaths[model1 + '_' + model2] = max_rsa.display_low_similarity_images(imagelist, sorted_indices[f'{model1}_{model2}'], maxdiffs[f'{model1}_{model2}'][:nb_seleted_categories], n_images=48,\n",
    "                                                      grid_cols=8, figsize=(20, 12),\n",
    "                                                      save_path=f'figures/compactness/subset_vgg/{savename}.png')"
   ],
   "id": "1080db44257b1eb9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import glob\n",
    "listsamples = glob.glob(f'results/categories_sim_samples*{dataset}.npy')\n",
    "nb_cols = 5\n",
    "fig, subs = plt.subplots(nrows = 2, ncols = nb_cols, figsize = (25,10), sharex = True, sharey = True)\n",
    "for f, file in enumerate(listsamples):\n",
    "    sample = np.load(file)\n",
    "    hist, bin_edges = np.histogram(sample, 100)\n",
    "    subs[int(f/nb_cols), f%nb_cols].bar(bin_edges[:-1],hist/max(hist), width = bin_edges[1] - bin_edges[0], linewidth = 0, align = 'edge')\n",
    "    #subs[f//5, f%5].legend()\n",
    "    subs[int(f/nb_cols), f%nb_cols].set_xlabel('Similarity')\n",
    "    subs[int(f/nb_cols), f%nb_cols].set_ylabel('Density')\n",
    "    subs[int(f/nb_cols), f%nb_cols].set_title(f'{file.split('_')[-3]}_{file.split('_')[-2]}')\n",
    "    name = f'{file.split('_')[-3]}_{file.split('_')[-2]}'\n",
    "    subs[int(f/nb_cols), f%nb_cols].vlines(cat_similarities[name],0,1, 'g')\n",
    "    subs[int(f/nb_cols), f%nb_cols].vlines(cat_similarities_compact[name],0,1, 'r')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(f'figures/compactness/categories_selectionVSdistribution_{dataset}.png')\n",
    "plt.close()"
   ],
   "id": "3f8d54f348064cef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "listpickles_ecoLennyTest = glob.glob(f'/home/alban/Documents/results_image_selection/{dataset}_*.pkl')\n",
    "\n",
    "RESULTS = {}\n",
    "for p, pkl in enumerate(listpickles_ecoLennyTest):\n",
    "    name = pkl.split('/')[-1][:-4]\n",
    "    f = open(pkl, \"rb\")\n",
    "    RESULTS[name] = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "list_names = [k for k in RESULTS.keys()]\n",
    "similarities_compactness = {}\n",
    "\n",
    "similarities_compactness = [RESULTS[f'{dataset}_Truenormalize_silhouette_score_normalizedDiff_pearson']['similarity_dict'][pair]['similarity'] for pair in\n",
    "                      RESULTS[f'{dataset}_Truenormalize_silhouette_score_normalizedDiff_pearson']['similarity_dict'].keys()]"
   ],
   "id": "c9042710785b5c2b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "## Looking at selections\n",
    "imagelist = [img.replace('/raid/leonard_vandyck/datasets/genloc/', '/home/alban/Documents/ecoLennyTest/') for img in imagelist]\n",
    "#imagelist = [img.replace('/raid/shared/datasets/visoin/ecoset/', '/home/alban/Documents/ecoset/') for img in imagelist]\n",
    "imagespaths = {}\n",
    "for i, model1 in enumerate(models[:-1]):\n",
    "    for j, model2 in enumerate(models[i+1:]):\n",
    "        savename = f'Truenormalize_Fisher_discriminant_corr_{arch}_{model1}_{model2}_ecoLennyTest'\n",
    "        images, imagespaths[model1 + '_' + model2] = max_rsa.display_low_similarity_images(imagelist, sorted_indices[f'{model1}_{model2}'], maxdiffs[f'{model1}_{model2}'][:nb_seleted_categories], n_images=48,\n",
    "                                                      grid_cols=8, figsize=(20, 12),\n",
    "                                                      save_path=f'figures/compactness/subset_vgg/{savename}.png')"
   ],
   "id": "a91c14712e2a3163",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "maxdiffs[:nb_seleted_categories]",
   "id": "23ab96b972fd8fec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "imagelist",
   "id": "a20776914397fdb5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "RDMs = {}\n",
    "metric = 'L2squared'\n",
    "for i, model in enumerate(models):\n",
    "    print(model)\n",
    "    RDMs[model] = rsa.compute_RDMs(activations[model], metric = metric, display = False, title = f'{model}_{metric}')"
   ],
   "id": "8d081628f8724375",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mean_RDM = RDMs.copy()\n",
    "for model in models:\n",
    "    mean_RDM[model] = RDMs[model].reshape(len(listcat), nb_per_cat, len(listcat), nb_per_cat)\n",
    "    mean_RDM[model] = mean_RDM[model].transpose(0, 2, 1, 3)\n",
    "    mean_RDM[model] = mean_RDM[model].mean(axis = (2,3))\n",
    "\n",
    "fig, subs = plt.subplots(1,2, sharex=True, sharey=True)\n",
    "subs[0].imshow(mean_RDM[submodels[0]], cmap='gray')\n",
    "subs[1].imshow(mean_RDM[submodels[1]], cmap='gray')\n",
    "subs[0].axis('off')\n",
    "subs[1].axis('off')\n",
    "fig.suptitle(f'{rsa.Compute_sim_RDMs(mean_RDM[submodels[0]], mean_RDM[submodels[1]], metric = 'pearson')}')\n",
    "fig.tight_layout()"
   ],
   "id": "1373d2404e38a291",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e434fb53f677e306",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "bd7697374011838d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "955746ffd7427021",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "fe97521aca52f9dd",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
